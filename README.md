# ðŸ¤– AI-Powered Support Assistant

A full-stack, AI-driven customer support assistant built with React, Node.js, and Google's Gemini API. This application utilizes Retrieval-Augmented Generation (RAG) through vector embeddings to provide highly accurate, document-based answers while strictly preventing hallucinations.

## ðŸŒŸ Key Features & Bonus Implementations
- **Strict Document-Based Answering (RAG):** Uses `gemini-embedding-001` to vectorize `docs.json` and perform cosine similarity mathematical searches. It only feeds the most relevant chunks to the LLM, ensuring high scalability.
- **Agentic Security Middleware:** Includes a custom Express middleware that detects and rejects Prompt Injection attacks (e.g., "ignore all previous instructions") before they reach the LLM.
- **Markdown UI Rendering:** The React frontend parses and renders Markdown beautifully for a premium user experience.
- **Persistent Session Memory:** Uses UUIDs and `localStorage` on the frontend, mapped to a relational SQLite database on the backend, preserving the last 5 conversation pairs for LLM context.
- **Rate Limiting:** IP-based rate limiting prevents API abuse.

## ðŸ§  Tech Stack
- **Frontend:** React.js (Vite), Axios, UUID, React-Markdown
- **Backend:** Node.js, Express.js, SQLite3
- **AI Integration:** Google Generative AI (`gemini-2.5-flash` for chat, `gemini-embedding-001` for vectorization)

---

## ðŸš€ Setup & Installation

### 1. Clone the Repository
\`\`\`bash
git clone https://github.com/yourusername/ai-support-assistant.git
cd ai-support-assistant
\`\`\`

### 2. Backend Setup
\`\`\`bash
cd backend
npm install
\`\`\`
Create a `.env` file in the `backend` directory based on the `.env.example`:
\`\`\`env
PORT=5000
GEMINI_API_KEY=your_gemini_api_key_here
\`\`\`
Start the backend server:
\`\`\`bash
npm run dev
\`\`\`

### 3. Frontend Setup
Open a new terminal window:
\`\`\`bash
cd frontend
npm install
npm run dev
\`\`\`
Navigate to `http://localhost:5173` in your browser.

---

## ðŸ—„ï¸ Database Schema (SQLite)

The application uses a relational schema to tie messages to specific user sessions.

**Table: `sessions`**
| Column | Type | Notes |
| :--- | :--- | :--- |
| `id` | TEXT | Primary Key (UUID generated by frontend) |
| `created_at` | DATETIME | Default `CURRENT_TIMESTAMP` |
| `updated_at` | DATETIME | Updates on new messages |

**Table: `messages`**
| Column | Type | Notes |
| :--- | :--- | :--- |
| `id` | INTEGER | Primary Key, Autoincrement |
| `session_id` | TEXT | Foreign Key targeting `sessions(id)` |
| `role` | TEXT | Enforced via CHECK constraint: `'user'` or `'assistant'` |
| `content` | TEXT | The raw message text |
| `created_at` | DATETIME | Default `CURRENT_TIMESTAMP` |

---

## ðŸ”Œ API Documentation

### A. Chat Endpoint
**`POST /api/chat`**
Processes the user message, retrieves context via vector search, calls the LLM, and saves the interaction.
- **Request Body:**
  \`\`\`json
  {
    "sessionId": "uuid-string-here",
    "message": "How can I reset my password?"
  }
  \`\`\`
- **Success Response (200 OK):**
  \`\`\`json
  {
    "reply": "Users can reset password from Settings > Security.",
    "tokensUsed": 142
  }
  \`\`\`
- **Error Response (403 Forbidden - Security Alert):**
  \`\`\`json
  {
    "reply": "Security Alert: Prompt injection attempt detected and blocked.",
    "tokensUsed": 0
  }
  \`\`\`

### B. Fetch Conversation
**`GET /api/conversations/:sessionId`**
Returns all messages for a specific session in chronological order.

### C. List Sessions
**`GET /api/sessions`**
Returns a list of all active sessions, ordered by `lastUpdated`.

---

## ðŸ¤” Assumptions & Design Decisions
1. **Vector Embeddings over Full-Text Injection:** Assuming `docs.json` could grow substantially in a real-world scenario, I opted for Cosine Similarity search over injecting the entire document payload into the system prompt to save tokens and improve LLM accuracy.
2. **Rate Limiting:** Set to 100 requests per 15-minute window per IP. This provides ample headroom for normal support queries while preventing malicious spam.
3. **Context Window:** The backend dynamically slices the SQLite query to limit history to the last 10 messages (5 user/assistant pairs) to maintain context without overloading the token limit.
